{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# IPD et Machine learning\n",
    "\n",
    "Auteur : Philippe Mathieu, CRISTAL Lab, SMAC Team, University of Lille, email : philippe.mathieu@univ-lille.fr\n",
    "\n",
    "Contributeurs : Louisa Fodil (CRISTAL/SMAC), Céline Petitpré (CRISTAL/SMAC)\n",
    "\n",
    "Creation : 18/01/2018\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Le Dilemme itéré du prisonnier permet d'exprimer une infinité de stratégies. Il est naturel de se demander laquelle est la meilleure ? Malheureusement il n'y a pas de stratégie meilleure dans l'absolu. On ne peut par exemple pas jouer optimalement contre `All_D` et `Spiteful`. Ceci est du au fait qu'il s'agit d'un jeu simultané, et qu'au premier coup, on ne connait pas encore son adversaire. Il faut donc choisir. Il y a par contre des stratégie plus \"robustes\" que d'autres, au sens où elles sont toujours efficaces dans des environnements variés.\n",
    "Les compétitions écologiques et les sous-classes sont des moyens de faire varier légèrement l'environnement.\n",
    "Reste maintenant à trouver de bonnes stratégies.\n",
    "Nous cherchons ici à savoir quels sont les outils qui nous permettent d'identifier de nouvelles stratégies robustes et notamment si les techniques d'Intelligence Artificielles peuvent nous être utiles pour les mettre en évidence.\n",
    "\n",
    "Dans ce notebook nous considérons comme acquis tous les outils précédemment créés et que nous savons évaluer un ensemble de stratégies par compétition écologique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All about simultaneous games (class Game with getNash(), getPareto(), getDominantStrategies())\n",
    "%run ../src/Game.py\n",
    "\n",
    "# All about ipd (Meeting, Tournament and Ecological classes)\n",
    "%matplotlib inline\n",
    "%run ../src/ipd.py\n",
    "\n",
    "# All about strategies (getPeriodics(n), getMem(X,Y), getClassicals())\n",
    "%run ../src/strategies.py\n",
    "\n",
    "# Comment utiliser ces modules ?\n",
    "\n",
    "# On crée le jeu du dilemme des prisonniers\n",
    "dip =[(3,3),(0,5),(5,0),(1,1)]   # Dilemme du prisonnier\n",
    "g = Game.Game(dip,['C','D'])\n",
    "\n",
    "# Un tournoi\n",
    "bag = []\n",
    "bag.append(Periodic('C'))\n",
    "bag.append(Periodic('D'))\n",
    "bag.append(Periodic('DDC'))\n",
    "bag.append(Periodic('CCD'))\n",
    "t=Tournament(g,bag,10)\n",
    "t.run()\n",
    "print(\"La matrice de scores du tournoi: \")\n",
    "print(t.matrix)\n",
    "\n",
    "\n",
    "# Une competition écologique\n",
    "gentille = Periodic(\"C\",\"All_C\")\n",
    "mechante = Periodic(\"D\",\"All_D\")\n",
    "eco = Ecological(g, [gentille, mechante])\n",
    "eco.run()\n",
    "print(\"Evolution de la population de la competition écologique : \")\n",
    "eco.drawPlot()\n",
    "print(\"Historique de la population de la competition écologique : \")\n",
    "print(eco.historic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprendre des stratégies\n",
    "\n",
    "A partir d'ici on considère qu'on sait faire des tournois , compétitions et sous-classes\n",
    "et qu'on a a disposition des ensembles de stratégies définies avec génotype. Ce qu'on cherche, c'est trouver des bonnes stratégies\n",
    "- methodes de monte carlo\n",
    "- algos génétiques\n",
    "- et tout ce que l'on peut faire de mieux (jouer sur les fonctions de fitness , faire du reinforcement learning, des reseaux de neurones etc ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methode de Monte-Carlo\n",
    "\n",
    "## Battre une classe mem\n",
    "Il est très souvent (toujours ?) possible de trouver une stratégie capable de battre toutes celles d'une classe donnée dans une classe d'ordre supérieur. Par exemple trouver une `Mem(2,2)` capable de gagner dans l'ensemble des `Mem(1,1)`. La manière la plus simple d'en trouver une consiste à calculer un certain nombre de fois un génotype aléatoire d'une `Mem(2,2)`, de l'évaluer dans `Mem(1,1)`, de regarder son classement et de ne conserver que celle qui a obtenu le meilleur classement. C'est le cas de `Mem(2,2,'CCCCDDDCCDCDDDDDDD')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FindBest:\n",
    "    def __init__(self, game):\n",
    "        self.game = game\n",
    "\n",
    "    def generate_random_genotype(self, x, y):\n",
    "        N = max(x,y) + 2**(x+y)\n",
    "        genotype = \"\"\n",
    "        for i in range (N):\n",
    "            genotype += random.choice(self.game.actions)\n",
    "        return genotype\n",
    "\n",
    "    def random_selection(self, x, y, nb_tests, soupe):\n",
    "        d = dict()\n",
    "        for n in range(nb_tests):\n",
    "            genotype = self.generate_random_genotype(x,y)\n",
    "            strat = Mem(x,y,genotype)\n",
    "            eco = Ecological(self.game, soupe+[strat])\n",
    "            eco.run()\n",
    "            d[genotype] = eco.historic.columns.tolist().index(strat.name)\n",
    "        return sorted(d.items(), key=lambda t: t[1])\n",
    "\n",
    "\n",
    "gen = FindBest(g)\n",
    "soupe = getMem(1,1)\n",
    "print(gen.random_selection(2,2,10,soupe)[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation de la stratégie trouvée précédemment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag3=getMem(1,1)\n",
    "e2=Ecological(g,bag3+[Mem(2,2,'CCCCDDDCCDCDDDDDDD')])\n",
    "e2.run()\n",
    "e2.drawPlot(None,4)\n",
    "evol=e2.historic\n",
    "print(evol.iloc[-1][evol.iloc[-1]>0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mais à cause de l'overfitting, elle est peu robuste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour mesurer la robustesse, on regarde le classement de cette stratégie dans les sous-classes de taille n-1.\n",
    "res = subClassesWithOneStrat(bag3, len(bag3)-1 , Mem(2,2,'CCCCDDDCCDCDDDDDDD'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algo Génétique\n",
    "\n",
    "Les techniques de Monte-Carlo sont simples à comprendre mais lentes à converger. Les algorithmes génétiques sont bien plus performants pour ce type de rechercher. On peut par exemple utiliser la librairie [Deap](https://deap.readthedocs.io/en/master/).\n",
    "\n",
    "Un algo génétique a pour objectif de trouver un bon génotype dans un ensemble des phénotypes possible. Il démarre initialement d'une liste de taille fixée, d'individus pris au hasard. Il effectue plusieurs tours de calculs à partir de cette liste. A chaque tour, il élimine de la liste les individus les plus mauvais calculés selon une fonction de fitness (`evaluateInd`). Il complète alors sa liste avec de nouveaux individus obtenus à l'aide de deux opérations élémentaires : la mutation (`myMutation`) et le crossing over (`cxTwoPoint (définie dans deap)`) pour toujours avoir une liste de même taille.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from deap import creator, base, tools, algorithms\n",
    "import random\n",
    "import time\n",
    "\n",
    "class Genetic:\n",
    "\n",
    "    def __init__(self, game, x, y, soupe, option = \"tournament\"):\n",
    "        self.game = game\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.soupe = soupe\n",
    "        self.option = option\n",
    "\n",
    "\n",
    "    def createEnv(self):\n",
    "        creator.create(\"FitnessMax\", base.Fitness, weights=(-1.0,)) #-1.0 in order to minimize , 1.0 to maximize\n",
    "        creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "        toolbox = base.Toolbox()\n",
    "        toolbox.register(\"random_action\", random.choice, \"CD\")\n",
    "        DIM = max(self.x,self.y) + 2**(self.x+self.y)\n",
    "        toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.random_action, n=DIM)\n",
    "        toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "        toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "        toolbox.register(\"select\", tools.selBest)\n",
    "        if self.option == \"tournament\" : \n",
    "            toolbox.register(\"evaluate\", self.evaluateIndTournament)\n",
    "        elif self.option == \"ecological\":\n",
    "            toolbox.register(\"evaluate\", self.evaluateIndEcological)        \n",
    "        toolbox.register(\"mutate\", self.myMutation)\n",
    "        return toolbox\n",
    "\n",
    "\n",
    "    \n",
    "    def runEvolutionnary(self, toolbox, pop, parents, children):\n",
    "        \"\"\"\n",
    "        parents : The number of individuals to select for the next generation.\n",
    "        children : The number of children to produce at each generation.\n",
    "        \"\"\"\n",
    "        # The probability that an offspring is produced by crossover.\n",
    "        cxpb = 0.8\n",
    "        # The probability that an offspring is produced by mutation.\n",
    "        mutpb = 0.05\n",
    "        # The number of generation.\n",
    "        ngen = 1\n",
    "        pop = toolbox.population(n=pop)\n",
    "        fit = math.inf\n",
    "        while (fit > 1):\n",
    "            algorithms.eaMuPlusLambda(pop, toolbox, parents, children, cxpb, mutpb, ngen)\n",
    "            # The algo stops when one individual becomes first in the Ecological ranking (in soup)\n",
    "            top = sorted(pop, key=lambda x:x.fitness.values[0])[-1]\n",
    "            fit = top.fitness.values[0]\n",
    "            print('Classement : '+str(int(fit)))\n",
    "        print('Stratégie gagnante : '+self.__str__(top))\n",
    "        return self.__str__(top)\n",
    "\n",
    "\n",
    "    def __str__(self,individual):\n",
    "        s = \"\"\n",
    "        for i in range(len(individual)):\n",
    "            s += individual[i]\n",
    "        return s\n",
    "\n",
    "\n",
    "    def evaluateIndEcological(self, individual):\n",
    "        genotype = self.__str__(individual)\n",
    "        strat = Mem(self.x, self.y, genotype)\n",
    "        eco = Ecological(self.game, self.soupe+[strat],  length=10)\n",
    "        eco.run()\n",
    "        return (float(eco.historic.columns.tolist().index(strat.name)+1),)\n",
    "    \n",
    "    def evaluateIndTournament(self, individual):\n",
    "        genotype = self.__str__(individual)\n",
    "        strat = Mem(self.x, self.y, genotype)\n",
    "        tournament = Tournament(self.game, self.soupe + [strat], 10)\n",
    "        tournament.run()\n",
    "        return (float(tournament.matrix['Total'].tolist().index(tournament.matrix['Total'][strat.name])+1),)\n",
    "\n",
    "\n",
    "    def myMutation(self, individual):\n",
    "        return (individual,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les algorithmes génétiques appliquées aux compétitions écologiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un exemple simple et rapide : trouver parmi Mem(1,1) une stratégie qui bat toujours \"Gentille\" et \"Méchante\" en compétition écologique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soupe = [Periodic(\"C\"),Periodic(\"D\")]\n",
    "gen = Genetic(g,1,1,soupe, \"ecological\")\n",
    "toolbox = gen.createEnv()\n",
    "t = time.time()\n",
    "gagnante = gen.runEvolutionnary(toolbox, 25, 12, 8)\n",
    "t2 = time.time()\n",
    "print(\"Le temps d'éxécution est de \" + str(t2-t) + \" secondes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On peut maintenant vérifier avec un graphe que la stratégie est bien gagnante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Ecological(g, soupe+[Mem(1,1,gagnante)])\n",
    "e.run()\n",
    "e.drawPlot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trouver la meilleure stratégie Mem(1,2) qui bat toutes les stratégies de Mem(1,1) (execution > 4H)\n",
    "Par exemple Mem(1,2,\"CCCCDDDDDD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soupe = getMem(1,1)\n",
    "gen = Genetic(g,1,2,soupe, \"ecological\")\n",
    "toolbox = gen.createEnv()\n",
    "t = time.time()\n",
    "gen.runEvolutionnary(toolbox, 25, 12, 8)\n",
    "t2 = time.time()\n",
    "print(\"Le temps d'éxécution est de \" + str(t2-t) + \" secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ici je vérifie et montre le résultat de l'algorithme génétique car l'algorithme génétique prends beaucoup de temps\n",
    "soupe = getMem(1,1)\n",
    "e = Ecological(g, soupe+[Mem(1,2,\"CCCCDDDDDD\")])\n",
    "e.run()\n",
    "e.drawPlot(None,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trouver la meilleure stratégie Mem(2,1) qui bat toutes les stratégies de Mem(1,1)(execution > 4H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soupe = getMem(1,1)\n",
    "gen = Genetic(g,2,1,soupe, \"ecological\")\n",
    "toolbox = gen.createEnv()\n",
    "t = time.time()\n",
    "gen.runEvolutionnary(toolbox, 25, 12, 8)\n",
    "t2 = time.time()\n",
    "print(\"Le temps d'éxécution est de \" + str(t2-t) + \" secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ici je vérifie et montre le résultat de l'algorithme génétique car l'algorithme génétique prends beaucoup de temps\n",
    "soupe = getMem(1,1)\n",
    "e = Ecological(g, soupe+[Mem(2,1,\"CCCDDDCDDD\")])\n",
    "e.run()\n",
    "e.drawPlot(None,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trouver la meilleure stratégie Mem(2,2) qui bat toutes les stratégies de Mem(1,1)(execution > 24H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soupe = getMem(1,1)\n",
    "gen = Genetic(g,2,2,soupe, \"ecological\")\n",
    "toolbox = gen.createEnv()\n",
    "t = time.time()\n",
    "gen.runEvolutionnary(toolbox, 25, 12, 8)\n",
    "t2 = time.time()\n",
    "print(\"Le temps d'éxécution est de \" + str(t2-t) + \" secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ici je vérifie et montre le résultat de l'algorithme génétique car l'algorithme génétique prends beaucoup de temps\n",
    "soupe = getMem(1,1)\n",
    "e = Ecological(g, soupe+[Mem(2,2,\"CCCCDDDDDCDDDDDDCD\")])\n",
    "e.run()\n",
    "e.drawPlot(None,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encore une fois les stratégies trouvées pour surperformer contre une classe donnée sont particulièrement bien adaptées à cette classe mais très peu robustes. En changeant ne serait-ce qu'une stratégie dans l'ensemble initiale, elles ne surperforment plus.\n",
    "On étudie ici une stratégie obtenue par algorithme génétique face à toutes les sous-classes de taille (n-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On cherche une stratégie gagnante dans Mem(1,1) qui bat gentille, méchante et softmajority.\n",
    "soupe = [Periodic(\"C\"),Periodic(\"D\"), SoftMajority()]\n",
    "gen = Genetic(g,1,1,soupe, \"ecological\")\n",
    "toolbox = gen.createEnv()\n",
    "gagnante = gen.runEvolutionnary(toolbox, 25, 12, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On regarde le classement de la stratégie gagnante trouvée dans les sous-classes de taile n-1.\n",
    "subClassesWithOneStrat(soupe, 2, Mem(1,1,gagnante))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit ici que la stratégie gagnante a une pire place égale à 2, ce qui montre bien qu'elle ne gagne pas tout le temps.\n",
    "On a fait de l'\"overfitting\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les algorithmes génétiques appliquées aux tournois\n",
    "Les tournois permettent d'avoir des résultats beaucoup plus rapidement qu'avec les compétitions écologiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trouver la meilleure stratégie Mem(1,2) qui bat toutes les stratégies de Mem(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soupe = getMem(1,1)\n",
    "gen = Genetic(g,1,2,soupe, \"tournament\")\n",
    "toolbox = gen.createEnv()\n",
    "t = time.time()\n",
    "gen.runEvolutionnary(toolbox, 25, 12, 8)\n",
    "t2 = time.time()\n",
    "print(\"Le temps d'éxécution est de \" + str(t2-t) + \" secondes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trouver la meilleure stratégie Mem(2,1) qui bat toutes les stratégies de Mem(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soupe = getMem(1,1)\n",
    "gen = Genetic(g,2,1,soupe, \"tournament\")\n",
    "toolbox = gen.createEnv()\n",
    "t = time.time()\n",
    "gen.runEvolutionnary(toolbox, 25, 12, 8)\n",
    "t2 = time.time()\n",
    "print(\"Le temps d'éxécution est de \" + str(t2-t) + \" secondes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trouver la meilleure stratégie Mem(2,2) qui bat toutes les stratégies de Mem(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soupe = getMem(1,1)\n",
    "gen = Genetic(g,2,2,soupe, \"tournament\")\n",
    "toolbox = gen.createEnv()\n",
    "t = time.time()\n",
    "gagnante = gen.runEvolutionnary(toolbox, 25, 12, 8)\n",
    "t2 = time.time()\n",
    "print(\"Le temps d'éxécution est de \" + str(t2-t) + \" secondes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trouver la meilleure stratégie Mem(3,3) qui bat toutes les stratégies de Mem(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soupe = getMem(1,1)\n",
    "gen = Genetic(g,3,3,soupe, \"tournament\")\n",
    "toolbox = gen.createEnv()\n",
    "t = time.time()\n",
    "gagnante = gen.runEvolutionnary(toolbox, 25, 12, 8)\n",
    "t2 = time.time()\n",
    "print(\"Le temps d'éxécution est de \" + str(t2-t) + \" secondes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approche neuronale (Caffe ? TensorFlow ? Keras ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliographie\n",
    "\n",
    "- Robert Axelrod, The Evolution of Cooperation (New York: Basic Books, 1984).\n",
    "- JP Delahaye et P Mathieu. Des surprises dans le monde de la coopération. Pour la Science, numéro spécial \"Les mathématiques sociales\", pp 58-66, Juillet 1999.\n",
    "- Philippe Mathieu, Jean-Paul Delahaye. [New Winning Strategies for the Iterated Prisoner's Dilemma](http://jasss.soc.surrey.ac.uk/20/4/12.html). J. Artificial Societies and Social Simulation 20(4) (2017)\n",
    "- Philippe Mathieu, Jean-Paul Delahaye. New Winning Strategies for the Iterated Prisoner's Dilemma. AAMAS 2015: 1665-1666\n",
    "- Bruno Beaufils, Jean-Paul Delahaye et Philippe Mathieu. Our Meeting with Gradual : A good Strategy for the Itareted Prisoner’s Dilemma, in Intern. Cof. on Artificial Life V (ALIFE V), pp. 159- 165, 16-18 mai 1996, Nara (Japon).\n",
    "- Martin Nowak et K. Sigmund, TIT for TAT in Heterogeneous Populations, Nature, vol. 355, n° 16, pp. 250-253, janvier 1992."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
