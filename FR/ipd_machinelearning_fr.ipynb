{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# IPD et Machine learning\n",
    "\n",
    "Auteur : Philippe Mathieu, CRISTAL Lab, SMAC Team, University of Lille, email : philippe.mathieu@univ-lille.fr\n",
    "\n",
    "Contributeurs : Louisa Fodil (CRISTAL/SMAC), Céline Petitpré (CRISTAL/SMAC)\n",
    "\n",
    "Creation : 18/01/2018\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Le Dilemme itéré du prisonnier permet d'exprimer une infinité de stratégies. Il est naturel de se demander laquelle est la meilleure ? Malheureusement il n'y a pas de stratégie meilleure dans l'absolu. On ne peut par exemple pas jouer optimalement contre `All_D` et `Spiteful`. Ceci est du au fait qu'il s'agit d'un jeu simultané, et qu'au premier coup, on ne connait pas encore son adversaire. Il faut donc choisir. Il y a par contre des stratégie plus \"robustes\" que d'autres, au sens où elles sont toujours efficaces dans des environnements variés.\n",
    "Les compétitions écologiques et les sous-classes sont des moyens de faire varier légèrement l'environnement.\n",
    "Reste maintenant à trouver de bonnes stratégies.\n",
    "Nous cherchons ici à savoir quels sont les outils qui nous permettent d'identifier de nouvelles stratégies robustes et notamment si les techniques d'Intelligence Artificielles peuvent nous être utiles pour les mettre en évidence.\n",
    "\n",
    "Dans ce notebook nous considérons comme acquis tous les outils précédemment créés et que nous savons évaluer un ensemble de stratégies par compétition écologique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All about simultaneous games (class Game with getNash(), getPareto(), getDominantStrategies())\n",
    "%run ../src/Game.py\n",
    "\n",
    "# All about ipd (Meeting, Tournament and Ecological classes)\n",
    "%matplotlib inline\n",
    "%run ../src/ipd.py\n",
    "\n",
    "# All about strategies (getPeriodics(n), getMem(X,Y), getClassicals())\n",
    "%run ../src/strategies.py\n",
    "\n",
    "# Commebnt on fait un tournoi, comment on affiche la matrice, comment on connait le vainqueur et idem pour compet ecolo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprendre des stratégies\n",
    "\n",
    "A partir d'ici on considère qu'on sait faire des tournois , compétitions et sous-classes\n",
    "et qu'on a a disposition des ensembles de stratégies définies avec génotype. Ce qu'on cherche, c'est trouver des bonnes stratégies\n",
    "- methodes de monte carlo\n",
    "- algos génétiques\n",
    "- et tout ce que l'on peut faire de mieux (jouer sur les fonctions de fitness , faire du reinforcement learning, des reseaux de neurones etc ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methode de Monte-Carlo\n",
    "\n",
    "## Battre une classe mem\n",
    "Il est très souvent (toujours ?) possible de trouver une stratégie capable de battre toutes celles d'une classe donnée dans une classe d'ordre supérieur. Par exemple trouver une `Mem(2,2)` capable de gagner dans l'ensemble des `Mem(1,1)`. La manière la plus simple d'en trouver une consiste à calculer un certain nombre de fois un génotype aléatoire d'une `Mem(2,2)`, de l'évaluer dans `Mem(1,1)`, de regarder son classement et de ne conserver que celle qui a obtenu le meilleur classement. C'est le cas de `Mem(2,2,'CCCCDDDCCDCDDDDDDD')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FindBest:\n",
    "    def __init__(self, game):\n",
    "        self.game = game\n",
    "\n",
    "    def generate_random_genotype(self, x, y):\n",
    "        N = max(x,y) + 2**(x+y)\n",
    "        genotype = \"\"\n",
    "        for i in range (N):\n",
    "            genotype += random.choice(self.game.actions)\n",
    "        return genotype\n",
    "\n",
    "    def random_selection(self, x, y, nb_tests, soupe):\n",
    "        d = dict()\n",
    "        for n in range(nb_tests):\n",
    "            genotype = self.generate_random_genotype(x,y)\n",
    "            strat = Mem(x,y,genotype)\n",
    "            eco = Ecological(self.game, soupe+[strat])\n",
    "            eco.run()\n",
    "            d[genotype] = eco.historic.columns.tolist().index(strat.name)\n",
    "        return sorted(d.items(), key=lambda t: t[1])\n",
    "\n",
    "\n",
    "dip =[(3,3),(0,5),(5,0),(1,1)]\n",
    "g = Game.Game(dip,['C','D'])\n",
    "gen = FindBest(g)\n",
    "soupe = getMem(1,1)\n",
    "print(gen.random_selection(2,2,10,soupe)[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Battre Mem(1,1) par Mem(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag3=getMem(1,1)\n",
    "e2=Ecological(g,bag3+[Mem(2,2,'CCCCDDDCCDCDDDDDDD')])\n",
    "e2.run()\n",
    "e2.drawPlot(None,4)\n",
    "evol=e2.historic\n",
    "print(evol.iloc[-1][evol.iloc[-1]>0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algo Génétique\n",
    "\n",
    "Les techniques de Monte-Carlo sont simples à comprendre mais lentes à converger. Les algorithmes génétiques sont bien plus performants pour ce type de rechercher. A l'aide de la librairie [Deap](https://deap.readthedocs.io/en/master/), trouver la meilleure stratégie `mem(3,3)` qui est la meilleure face à `mem(1,1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from deap import creator, base, tools, algorithms\n",
    "import random\n",
    "import time\n",
    "\n",
    "class Genetic:\n",
    "\n",
    "    def __init__(self, game, x, y, soupe):\n",
    "        self.game = game\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.soupe = soupe\n",
    "\n",
    "\n",
    "    def createEnv(self):\n",
    "        creator.create(\"FitnessMax\", base.Fitness, weights=(-1.0,)) #-1.0 in order to minimize , 1.0 to maximize\n",
    "        creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "        toolbox = base.Toolbox()\n",
    "        toolbox.register(\"random_action\", random.choice, \"CD\")\n",
    "        DIM = max(self.x,self.y) + 2**(self.x+self.y)\n",
    "        toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.random_action, n=DIM)\n",
    "        toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "        toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "        toolbox.register(\"select\", tools.selBest)\n",
    "        toolbox.register(\"evaluate\", self.evaluateInd)\n",
    "        toolbox.register(\"mutate\", self.myMutation)\n",
    "        return toolbox\n",
    "\n",
    "\n",
    "    \n",
    "    def runEvolutionnary(self, toolbox, pop, parents, children):\n",
    "        \"\"\"\n",
    "        parents : The number of individuals to select for the next generation.\n",
    "        children : The number of children to produce at each generation.\n",
    "        \"\"\"\n",
    "        # The probability that an offspring is produced by crossover.\n",
    "        cxpb = 0.8\n",
    "        # The probability that an offspring is produced by mutation.\n",
    "        mutpb = 0.05\n",
    "        # The number of generation.\n",
    "        ngen = 1\n",
    "        pop = toolbox.population(n=pop)\n",
    "        fit = math.inf\n",
    "        while (fit > 1):\n",
    "            algorithms.eaMuPlusLambda(pop, toolbox, parents, children, cxpb, mutpb, ngen)\n",
    "            top = sorted(pop, key=lambda x:x.fitness.values[0])[-1]\n",
    "            fit = top.fitness.values[0]\n",
    "            print('Classement : '+str(int(fit)))\n",
    "        print('Stratégie gagnante : '+self.__str__(top))\n",
    "        return self.__str__(top)\n",
    "\n",
    "\n",
    "    def __str__(self,individual):\n",
    "        s = \"\"\n",
    "        for i in range(len(individual)):\n",
    "            s += individual[i]\n",
    "        return s\n",
    "\n",
    "\n",
    "    def evaluateInd(self, individual):\n",
    "        genotype = self.__str__(individual)\n",
    "        strat = Mem(self.x, self.y, genotype)\n",
    "        eco = Ecological(self.game, self.soupe+[strat],  length=100)\n",
    "        eco.run()\n",
    "        return (float(eco.historic.columns.tolist().index(strat.name)+1),)\n",
    "\n",
    "\n",
    "    def myMutation(self, individual):\n",
    "        return (individual,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un exemple simple et rapide : trouver parmi Mem(1,1) une stratégie qui bat toujours \"Gentille\" et \"Méchante\" en compétition écologique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dip =[(3,3),(0,5),(5,0),(1,1)]   # Dilemme du prisonnier\n",
    "g = Game.Game(dip,['C','D'])\n",
    "soupe = [Periodic(\"C\"),Periodic(\"D\")]\n",
    "gen = Genetic(g,1,1,soupe)\n",
    "toolbox = gen.createEnv()\n",
    "t = time.time()\n",
    "gagnante = gen.runEvolutionnary(toolbox, 25, 12, 8)\n",
    "t2 = time.time()\n",
    "print(\"Le temps d'éxécution est de \" + str(t2-t) + \" secondes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On peut maintenant vérifier avec un graphe que la stratégie est bien gagnante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Ecological(g, soupe+[Mem(1,1,gagnante)])\n",
    "e.run()\n",
    "e.drawPlot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un deuxieme exemple simple et rapide : Trouver parmi Mem(1,1) la stratégie qui bat Gradual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Genetic(g,1,1,[Gradual()])\n",
    "toolbox = gen.createEnv()\n",
    "gen.runEvolutionnary(toolbox, 25, 12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trouver la meilleure stratégie Mem(1,2) qui bat toutes les stratégies de Mem(1,1) (code très long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dip =[(3,3),(0,5),(5,0),(1,1)]   # Dilemme du prisonnier\n",
    "g = Game.Game(dip,['C','D'])\n",
    "soupe = getMem(1,1)\n",
    "gen = Genetic(g,1,2,soupe)\n",
    "toolbox = gen.createEnv()\n",
    "t = time.time()\n",
    "gen.runEvolutionnary(toolbox, 25, 12, 8)\n",
    "t2 = time.time()\n",
    "print(\"Le temps d'éxécution est de \" + str(t2-t) + \" secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ici je vérifie et montre le résultat de l'algorithme génétique car l'algorithme génétique prends beaucoup de temps\n",
    "soupe = getMem(1,1)\n",
    "e = Ecological(g, soupe+[Mem(1,2,\"CCCCDDDDDD\")])\n",
    "e.run()\n",
    "e.drawPlot(None,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trouver la meilleure stratégie Mem(2,1) qui bat toutes les stratégies de Mem(1,1)(code très long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dip =[(3,3),(0,5),(5,0),(1,1)]   # Dilemme du prisonnier\n",
    "g = Game.Game(dip,['C','D'])\n",
    "soupe = getMem(1,1)\n",
    "gen = Genetic(g,2,1,soupe)\n",
    "toolbox = gen.createEnv()\n",
    "t = time.time()\n",
    "gen.runEvolutionnary(toolbox, 25, 12, 8)\n",
    "t2 = time.time()\n",
    "print(\"Le temps d'éxécution est de \" + str(t2-t) + \" secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ici je vérifie et montre le résultat de l'algorithme génétique car l'algorithme génétique prends beaucoup de temps\n",
    "soupe = getMem(1,1)\n",
    "e = Ecological(g, soupe+[Mem(2,1,\"CCCDDDCDDD\")])\n",
    "e.run()\n",
    "e.drawPlot(None,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trouver la meilleure stratégie Mem(2,2) qui bat toutes les stratégies de Mem(1,1)(code très long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dip =[(3,3),(0,5),(5,0),(1,1)]   # Dilemme du prisonnier\n",
    "g = Game.Game(dip,['C','D'])\n",
    "soupe = getMem(1,1)\n",
    "gen = Genetic(g,2,2,soupe)\n",
    "toolbox = gen.createEnv()\n",
    "t = time.time()\n",
    "gen.runEvolutionnary(toolbox, 25, 12, 8)\n",
    "t2 = time.time()\n",
    "print(\"Le temps d'éxécution est de \" + str(t2-t) + \" secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ici je vérifie et montre le résultat de l'algorithme génétique car l'algorithme génétique prends beaucoup de temps\n",
    "soupe = getMem(1,1)\n",
    "e = Ecological(g, soupe+[Mem(2,2,\"CCCCDDDDDCDDDDDDCD\")])\n",
    "e.run()\n",
    "e.drawPlot(None,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malheureusement les stratégies trouvées pour surperformer contre une classe donnée sont particulièrement bien adaptées à cette classe mais très peu robustes. En changeant ne serait-ce qu'une stratégie dans l'ensemble initiale, elles ne surperforment plus.\n",
    "On étudie ici une stratégie obtenue par algorithme génétique face à toutes les sous-classes de taille (n-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On cherche une stratégie gagnante dans Mem(1,1) qui bat gentille, méchante et softmajority.\n",
    "dip =[(3,3),(0,5),(5,0),(1,1)]   # Dilemme du prisonnier\n",
    "g = Game.Game(dip,['C','D'])\n",
    "soupe = [Periodic(\"C\"),Periodic(\"D\"), SoftMajority()]\n",
    "gen = Genetic(g,1,1,soupe)\n",
    "toolbox = gen.createEnv()\n",
    "gagnante = gen.runEvolutionnary(toolbox, 25, 12, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On regarder le classement de la stratégie gagnante trouvée dans les sous-classes de taile n-1.\n",
    "subClassesWithOneStrat(soupe, 2, Mem(1,1,gagnante))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit ici que la stratégie gagnante a une pire place égale à 2, ce qui montre bien qu'elle ne gagne pas tout le temps.\n",
    "On a fait de l'\"overfitting\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approche neuronale (Caffe ? TensorFlow ? Keras ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliographie\n",
    "\n",
    "- Robert Axelrod, The Evolution of Cooperation (New York: Basic Books, 1984).\n",
    "- JP Delahaye et P Mathieu. Des surprises dans le monde de la coopération. Pour la Science, numéro spécial \"Les mathématiques sociales\", pp 58-66, Juillet 1999.\n",
    "- Philippe Mathieu, Jean-Paul Delahaye. [New Winning Strategies for the Iterated Prisoner's Dilemma](http://jasss.soc.surrey.ac.uk/20/4/12.html). J. Artificial Societies and Social Simulation 20(4) (2017)\n",
    "- Philippe Mathieu, Jean-Paul Delahaye. New Winning Strategies for the Iterated Prisoner's Dilemma. AAMAS 2015: 1665-1666\n",
    "- Bruno Beaufils, Jean-Paul Delahaye et Philippe Mathieu. Our Meeting with Gradual : A good Strategy for the Itareted Prisoner’s Dilemma, in Intern. Cof. on Artificial Life V (ALIFE V), pp. 159- 165, 16-18 mai 1996, Nara (Japon).\n",
    "- Martin Nowak et K. Sigmund, TIT for TAT in Heterogeneous Populations, Nature, vol. 355, n° 16, pp. 250-253, janvier 1992."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
