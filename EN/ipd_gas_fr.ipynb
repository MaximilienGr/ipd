{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# IPD et Machine learning\n",
    "\n",
    "Auteur : Philippe Mathieu, CRISTAL Lab, SMAC Team, University of Lille, email : philippe.mathieu@univ-lille.fr\n",
    "\n",
    "Contributeurs : Louisa Fodil (CRISTAL/SMAC), Céline Petitpré (CRISTAL/SMAC)\n",
    "\n",
    "Creation : 18/01/2018\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Le Dilemme itéré du prisonnier permet d'exprimer une infinité de stratégies. Il est naturel de se demander laquelle est la meilleure ? Malheureusement il n'y a pas de stratégie meilleure dans l'absolu. On ne peut par exemple pas jouer optimalement contre `All_D` et `Spiteful`. Ceci est du au fait qu'il s'agit d'un jeu simultané, et qu'au premier coup, on ne connait pas encore son adversaire. Il faut donc choisir. Il y a par contre des stratégie plus \"robustes\" que d'autres, au sens où elles sont toujours efficaces dans des environnements variés.\n",
    "Les compétitions écologiques et les sous-classes sont des moyens de faire varier légèrement l'environnement.\n",
    "Reste maintenant à trouver de bonnes stratégies.\n",
    "Nous cherchons ici à savoir quels sont les outils qui nous permettent d'identifier de nouvelles stratégies robustes et notamment si les techniques d'Intelligence Artificielles peuvent nous être utiles pour les mettre en évidence.\n",
    "\n",
    "Dans ce notebook nous considérons comme acquis tous les outils précédemment créés et que nous savons évaluer un ensemble de stratégies par compétition écologique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All about simultaneous games (class Game with getNash(), getPareto(), getDominantStrategies())\n",
    "%run ../src/game.py\n",
    "\n",
    "# All about ipd (Meeting, Tournament and Ecological classes)\n",
    "%matplotlib inline\n",
    "%run ../src/ipd.py\n",
    "\n",
    "# All about strategies (getPeriodics(), getMem(X,Y), getClassicals() with run() method)\n",
    "%run ../src/strategies.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprendre des stratégies\n",
    "\n",
    "A partir d'ici on considère qu'on sait faire des tournois , compétitions et sous-classes\n",
    "et qu'on a a disposition des ensembles de stratégies définies avec génotype. Ce qu'on cherche, c'est trouver des bonnes stratégies\n",
    "- methodes de monte carlo\n",
    "- algos génétiques\n",
    "- et tout ce que l'on peut faire de mieux (jouer sur les fonctions de fitness , faire du reinforcement learning, des reseaux de neurones etc ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methode de Monte-Carlo\n",
    "\n",
    "## Battre une classe mem\n",
    "Il est très souvent (toujours ?) possible de trouver une stratégie capable de battre toutes celles d'une classe données dans une classe d'ordre supérieur. Par exemple trouver une `Mem(2,2)` capable de gagner dans l'ensemble des `Mem(1,1)`. La manière la plus simple d'en trouver une consiste à calculer un certain nombre de fois un génotype aléatoire d'une `Mem(2,2)`, de l'évaluer dans `Mem(1,1)`, de regarder son classement et de ne conserver que celle qui a obtenu le meilleur classement. C'est le cas de `Mem(2,2,'CCCCDDDCCDCDDDDDDD')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FindBest:\n",
    "    def __init__(self, game):\n",
    "        self.game = game\n",
    "\n",
    "    def generate_random_genotype(self, x, y):\n",
    "        N = max(x, y) + 2 ** (x + y)\n",
    "        genotype = \"\"\n",
    "        for i in range(N):\n",
    "            genotype += random.choice(self.game.actions)\n",
    "        return genotype\n",
    "\n",
    "    def random_selection(self, x, y, nb_tests, soupe):\n",
    "        d = dict()\n",
    "        for n in range(nb_tests):\n",
    "            genotype = self.generate_random_genotype(x, y)\n",
    "            strat = Mem(x, y, genotype)\n",
    "            eco = Ecological(self.game, soupe + [strat])\n",
    "            eco.run()\n",
    "            d[genotype] = eco.historic.columns.tolist().index(strat.name)\n",
    "        return sorted(d.items(), key=lambda t: t[1])\n",
    "\n",
    "\n",
    "dip = [(3, 3), (0, 5), (5, 0), (1, 1)]\n",
    "g = Game(dip, [\"C\", \"D\"])\n",
    "gen = FindBest(g)\n",
    "bag = getAllMemory(1, 1)\n",
    "print(gen.random_selection(2, 2, 10, bag)[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Battre Mem(1,1) par Mem(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag3 = getAllMemory(1, 1)\n",
    "e2 = Ecological(g, bag3 + [Mem(2, 2, \"CCCCDDDCCDCDDDDDDD\")])\n",
    "e2.run()\n",
    "e2.drawPlot(None, 4)\n",
    "evol = e2.historic\n",
    "print(evol.iloc[-1][evol.iloc[-1] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algo Génétique\n",
    "\n",
    "Les techniques de Monte-Carlo sont simples à comprendre mais lentes à converger. Les algorithmes génétiques sont bien plus performants pour ce type de rechercher. A l'aide de la librairie [Deap](https://deap.readthedocs.io/en/master/), trouver la meilleure stratégie `mem(3,3)` qui est la meilleure face à `mem(1,1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malheureusement les stratégies trouvées pour surperformer contre une classe donnée sont particulièrement bien adaptées à cette classe mais très peu robustes. En changeant ne serait-ce qu'une stratégie dans l'ensemble initiale, elles ne surperforment plus.\n",
    "On étudie ici une stratégie obtenue par algorithme génétique face à toutes les sous-classes de taille (n-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approche neuronale (Caffe ? TensorFlow ? Keras ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliographie\n",
    "\n",
    "- Robert Axelrod, The Evolution of Cooperation (New York: Basic Books, 1984).\n",
    "- JP Delahaye et P Mathieu. Des surprises dans le monde de la coopération. Pour la Science, numéro spécial \"Les mathématiques sociales\", pp 58-66, Juillet 1999.\n",
    "- Philippe Mathieu, Jean-Paul Delahaye. [New Winning Strategies for the Iterated Prisoner's Dilemma](http://jasss.soc.surrey.ac.uk/20/4/12.html). J. Artificial Societies and Social Simulation 20(4) (2017)\n",
    "- Philippe Mathieu, Jean-Paul Delahaye. New Winning Strategies for the Iterated Prisoner's Dilemma. AAMAS 2015: 1665-1666\n",
    "- Bruno Beaufils, Jean-Paul Delahaye et Philippe Mathieu. Our Meeting with Gradual : A good Strategy for the Itareted Prisoner’s Dilemma, in Intern. Cof. on Artificial Life V (ALIFE V), pp. 159- 165, 16-18 mai 1996, Nara (Japon).\n",
    "- Martin Nowak et K. Sigmund, TIT for TAT in Heterogeneous Populations, Nature, vol. 355, n° 16, pp. 250-253, janvier 1992."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
